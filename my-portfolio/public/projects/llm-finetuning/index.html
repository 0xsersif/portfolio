






<!doctype html>
<html
  lang="en"
  dir="ltr"
  class="scroll-smooth"
  data-default-appearance="light"
  data-auto-appearance="true"
><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="theme-color" content="#FFFFFF" />
  
  <title>LLM Fine-Tuning with LoRA/QLoRA &middot; 0xsersif // Full-Stack Developer & ML Engineer</title>
    <meta name="title" content="LLM Fine-Tuning with LoRA/QLoRA &middot; 0xsersif // Full-Stack Developer & ML Engineer" />
  
  
  
  
  
  <script
    type="text/javascript"
    src="http://localhost:1313/js/appearance.min.8a082f81b27f3cb2ee528df0b0bdc39787034cf2cc34d4669fbc9977c929023c.js"
    integrity="sha256-iggvgbJ/PLLuUo3wsL3Dl4cDTPLMNNRmn7yZd8kpAjw="
  ></script>
  
  
  
  
  
  
  
    
  
  
  <link
    type="text/css"
    rel="stylesheet"
    href="http://localhost:1313/css/main.bundle.min.b03e5259dac2760f0e3006bed7d970a363f24345198646bec6dbd9d2fb966a12.css"
    integrity="sha256-sD5SWdrCdg8OMAa&#43;19lwo2PyQ0UZhka&#43;xtvZ0vuWahI="
  />
  
  
  
  
  
  
  
  <meta
    name="description"
    content="
      Fine-tuned Llama 3 and Mistral on specialized datasets using parameter-efficient LoRA/QLoRA
    "
  />
  
  
  
  
    <link rel="canonical" href="http://localhost:1313/projects/llm-finetuning/" />
  
  
  
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png" />
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png" />
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png" />
    <link rel="manifest" href="/site.webmanifest" />
  
  
  
  
  
  
  
  
  <meta property="og:url" content="http://localhost:1313/projects/llm-finetuning/">
  <meta property="og:site_name" content="0xsersif // Full-Stack Developer & ML Engineer">
  <meta property="og:title" content="LLM Fine-Tuning with LoRA/QLoRA">
  <meta property="og:description" content="Fine-tuned Llama 3 and Mistral on specialized datasets using parameter-efficient LoRA/QLoRA">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="projects">
    <meta property="article:published_time" content="2026-01-20T00:00:00+00:00">
    <meta property="article:modified_time" content="2026-01-20T00:00:00+00:00">
    <meta property="article:tag" content="MLOps">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Fine-Tuning">
    <meta property="article:tag" content="LoRA">
    <meta property="article:tag" content="PyTorch">
    <meta property="article:tag" content="Hugging Face">

  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="LLM Fine-Tuning with LoRA/QLoRA">
  <meta name="twitter:description" content="Fine-tuned Llama 3 and Mistral on specialized datasets using parameter-efficient LoRA/QLoRA">

  
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Article",
    "articleSection": "Projects",
    "name": "LLM Fine-Tuning with LoRA\/QLoRA",
    "headline": "LLM Fine-Tuning with LoRA\/QLoRA",
    "description": "Fine-tuned Llama 3 and Mistral on specialized datasets using parameter-efficient LoRA\/QLoRA",
    "abstract": "\u003ch1 id=\u0022llm-fine-tuning-with-loraqlora\u0022 class=\u0022relative group\u0022\u003eLLM Fine-Tuning with LoRA\/QLoRA \u003cspan class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\u0022\u003e\u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022 style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#llm-fine-tuning-with-loraqlora\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\u003c\/span\u003e\u003c\/h1\u003e\u003ch2 id=\u0022project-overview\u0022 class=\u0022relative group\u0022\u003eProject Overview \u003cspan class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\u0022\u003e\u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022 style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#project-overview\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\u003c\/span\u003e\u003c\/h2\u003e\u003cp\u003eFine-tuned smaller open-source LLMs (\u003cstrong\u003eLlama 3\u003c\/strong\u003e, \u003cstrong\u003eMistral 7B\u003c\/strong\u003e) on specialized datasets (medical, legal, code) using \u003cstrong\u003eparameter-efficient tuning\u003c\/strong\u003e techniques (LoRA and QLoRA) to achieve domain-specific performance improvements.\u003c\/p\u003e\n\u003ch2 id=\u0022why-fine-tuning\u0022 class=\u0022relative group\u0022\u003eWhy Fine-Tuning? \u003cspan class=\u0022absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100\u0022\u003e\u003ca class=\u0022group-hover:text-primary-300 dark:group-hover:text-neutral-700\u0022 style=\u0022text-decoration-line: none !important;\u0022 href=\u0022#why-fine-tuning\u0022 aria-label=\u0022Anchor\u0022\u003e#\u003c\/a\u003e\u003c\/span\u003e\u003c\/h2\u003e\u003cp\u003ePre-trained LLMs struggle with:\u003c\/p\u003e",
    "inLanguage": "en",
    "url" : "http:\/\/localhost:1313\/projects\/llm-finetuning\/",
    "author" : {
      "@type": "Person",
      "name": "0xsersif"
    },
    "copyrightYear": "2026",
    "dateCreated": "2026-01-20T00:00:00\u002b00:00",
    "datePublished": "2026-01-20T00:00:00\u002b00:00",
    
    "dateModified": "2026-01-20T00:00:00\u002b00:00",
    
    "keywords": ["MLOps","LLM","Fine-Tuning","LoRA","PyTorch","Hugging Face"],
    
    "mainEntityOfPage": "true",
    "wordCount": "544"
  }
  </script>
    
    <script type="application/ld+json">
    {
   "@context": "https://schema.org",
   "@type": "BreadcrumbList",
   "itemListElement": [
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/",
       "name": "",
       "position": 1
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/projects/",
       "name": "Projects",
       "position": 2
     },
     {
       "@type": "ListItem",
       "item": "http://localhost:1313/categories/mlops/",
       "name": "MLOps",
       "position": 3
     },
     {
       "@type": "ListItem",
       "name": "Llm Fine Tuning With Lo Ra Qlo Ra",
       "position": 4
     }
   ]
 }
  </script>

  
  
    <meta name="author" content="0xsersif" />
  
  
  
  







  
  

  
  
</head>
<body
    class="m-auto flex h-screen max-w-7xl flex-col bg-neutral px-6 text-lg leading-7 text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"
  >
    <div id="the-top" class="absolute flex self-center">
      <a
        class="-translate-y-8 rounded-b-lg bg-primary-200 px-3 py-1 text-sm focus:translate-y-0 dark:bg-neutral-600"
        href="#main-content"
        ><span class="pe-2 font-bold text-primary-600 dark:text-primary-400">&darr;</span
        >Skip to main content</a
      >
    </div>
    
    
      <header class="py-6 font-semibold text-neutral-900 dark:text-neutral sm:py-10 print:hidden">
  <nav class="flex items-start justify-between sm:items-center">
    
    <div class="flex flex-row items-center">
      
  <a
    class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2"
    rel="me"
    href="/"
    >0xsersif // Full-Stack Developer &amp; ML Engineer</a
  >

    </div>
    
    
      <ul class="flex list-none flex-col text-end sm:flex-row">
        
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/about/"
                  title="About"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >About</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/projects/"
                  title="Projects"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Projects</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/philosophy/"
                  title="Development Philosophy &amp; MLOps Practices"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Philosophy</span
                    >
                  </a
                >
              
            </li>
          
            
            <li class="group mb-1 sm:mb-0 sm:me-7 sm:last:me-0.5">
              
                <a
                  href="/self-learning-path/"
                  title="Self-Learning Path"
                  
                  
                  ><span
                      class="decoration-primary-500 group-hover:underline group-hover:decoration-2 group-hover:underline-offset-2"
                      >Self-Learning Path</span
                    >
                  </a
                >
              
            </li>
          
          
        
      </ul>
    
  </nav>
</header>

    
    <div class="relative flex grow flex-col">
      <main id="main-content" class="grow">
        
  <article>
    <header class="max-w-prose">
      
      <h1 class="mb-8 mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">
        LLM Fine-Tuning with LoRA/QLoRA
      </h1>
      
        <div class="mb-10 text-base text-neutral-500 dark:text-neutral-400 print:hidden">
          





  
  



  

  
  
    
  

  

  

  

  


  <div class="flex flex-row flex-wrap items-center">
    
    
      <time datetime="2026-01-20 00:00:00 &#43;0000 UTC">20 January 2026</time>
    

    
    
  </div>

  
  


        </div>
      
      
    </header>
    <section class="prose mt-0 flex max-w-full flex-col dark:prose-invert lg:flex-row">
      
      <div class="min-h-0 min-w-0 max-w-prose grow">
        <h1 id="llm-fine-tuning-with-loraqlora" class="relative group">LLM Fine-Tuning with LoRA/QLoRA <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#llm-fine-tuning-with-loraqlora" aria-label="Anchor">#</a></span></h1><h2 id="project-overview" class="relative group">Project Overview <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#project-overview" aria-label="Anchor">#</a></span></h2><p>Fine-tuned smaller open-source LLMs (<strong>Llama 3</strong>, <strong>Mistral 7B</strong>) on specialized datasets (medical, legal, code) using <strong>parameter-efficient tuning</strong> techniques (LoRA and QLoRA) to achieve domain-specific performance improvements.</p>
<h2 id="why-fine-tuning" class="relative group">Why Fine-Tuning? <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#why-fine-tuning" aria-label="Anchor">#</a></span></h2><p>Pre-trained LLMs struggle with:</p>
<ul>
<li>Domain-specific terminology (medical diagnoses, legal precedents)</li>
<li>Proprietary knowledge not in training data</li>
<li>Consistently following custom formatting requirements</li>
</ul>
<p>Fine-tuning adapts the model to your specific use case without full retraining.</p>
<h2 id="methodology" class="relative group">Methodology <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#methodology" aria-label="Anchor">#</a></span></h2><h3 id="lora-low-rank-adaptation" class="relative group">LoRA (Low-Rank Adaptation) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#lora-low-rank-adaptation" aria-label="Anchor">#</a></span></h3><p>Instead of updating all model weights (expensive), LoRA injects <strong>trainable low-rank matrices</strong> into attention layers, reducing trainable parameters by <strong>99%</strong>.</p>
<h3 id="qlora-quantized-lora" class="relative group">QLoRA (Quantized LoRA) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#qlora-quantized-lora" aria-label="Anchor">#</a></span></h3><p>Further optimization:</p>
<ul>
<li>Quantize base model to 4-bit precision</li>
<li>Use NormalFloat4 for memory efficiency</li>
<li>Train LoRA adapters in higher precision</li>
</ul>
<p><strong>Result</strong>: Fine-tune 7B model on consumer GPU (24GB VRAM)</p>
<h2 id="tech-stack" class="relative group">Tech Stack <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#tech-stack" aria-label="Anchor">#</a></span></h2><ul>
<li><strong>PyTorch</strong> â€” Deep learning framework</li>
<li><strong>Hugging Face Transformers</strong> â€” Model loading and inference</li>
<li><strong>PEFT (Parameter-Efficient Fine-Tuning)</strong> â€” LoRA implementation</li>
<li><strong>bitsandbytes</strong> â€” 4-bit quantization</li>
<li><strong>Weights &amp; Biases (W&amp;B)</strong> â€” Experiment tracking and metrics</li>
<li><strong>Datasets</strong> â€” Data loading and preprocessing</li>
<li><strong>Accelerate</strong> â€” Distributed training support</li>
</ul>
<h2 id="training-pipeline" class="relative group">Training Pipeline <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#training-pipeline" aria-label="Anchor">#</a></span></h2><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#6272a4"># 1. Load quantized base model</span>
</span></span><span style="display:flex;"><span>model <span style="color:#ff79c6">=</span> AutoModelForCausalLM<span style="color:#ff79c6">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    <span style="color:#f1fa8c">&#34;mistralai/Mistral-7B-v0.1&#34;</span>,
</span></span><span style="display:flex;"><span>    load_in_4bit<span style="color:#ff79c6">=</span><span style="color:#ff79c6">True</span>,
</span></span><span style="display:flex;"><span>    bnb_4bit_compute_dtype<span style="color:#ff79c6">=</span>torch<span style="color:#ff79c6">.</span>float16
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 2. Configure LoRA</span>
</span></span><span style="display:flex;"><span>lora_config <span style="color:#ff79c6">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    r<span style="color:#ff79c6">=</span><span style="color:#bd93f9">16</span>, lora_alpha<span style="color:#ff79c6">=</span><span style="color:#bd93f9">32</span>, lora_dropout<span style="color:#ff79c6">=</span><span style="color:#bd93f9">0.05</span>,
</span></span><span style="display:flex;"><span>    target_modules<span style="color:#ff79c6">=</span>[<span style="color:#f1fa8c">&#34;q_proj&#34;</span>, <span style="color:#f1fa8c">&#34;v_proj&#34;</span>]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 3. Train with W&amp;B tracking</span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#ff79c6">=</span> SFTTrainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#ff79c6">=</span>model, train_dataset<span style="color:#ff79c6">=</span>dataset,
</span></span><span style="display:flex;"><span>    peft_config<span style="color:#ff79c6">=</span>lora_config
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#ff79c6">.</span>train()
</span></span></code></pre></div><h2 id="use-cases" class="relative group">Use Cases <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#use-cases" aria-label="Anchor">#</a></span></h2><h3 id="1-medical-diagnosis-assistant" class="relative group">1. Medical Diagnosis Assistant <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#1-medical-diagnosis-assistant" aria-label="Anchor">#</a></span></h3><ul>
<li><strong>Dataset</strong>: 10k+ medical case studies and diagnostic guidelines</li>
<li><strong>Improvement</strong>: 18% increase in diagnostic accuracy vs base model</li>
<li><strong>Deployment</strong>: HIPAA-compliant API on AWS</li>
</ul>
<h3 id="2-legal-contract-analysis" class="relative group">2. Legal Contract Analysis <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#2-legal-contract-analysis" aria-label="Anchor">#</a></span></h3><ul>
<li><strong>Dataset</strong>: 5k annotated legal contracts</li>
<li><strong>Improvement</strong>: 22% better clause identification</li>
<li><strong>Deployment</strong>: FastAPI endpoint with Docker</li>
</ul>
<h3 id="3-code-generation-python" class="relative group">3. Code Generation (Python) <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#3-code-generation-python" aria-label="Anchor">#</a></span></h3><ul>
<li><strong>Dataset</strong>: Proprietary internal codebase + documentation</li>
<li><strong>Improvement</strong>: 15% higher pass@1 on code completion tasks</li>
<li><strong>Deployment</strong>: Hugging Face Hub for team access</li>
</ul>
<h2 id="results--metrics" class="relative group">Results &amp; Metrics <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#results--metrics" aria-label="Anchor">#</a></span></h2><p>ðŸ“Š <strong>15% improvement in domain-specific F1-score</strong> over base model<br>
ðŸ“Š <strong>99% reduction in trainable parameters</strong> (LoRA efficiency)<br>
ðŸ“Š <strong>4x faster training</strong> compared to full fine-tuning<br>
ðŸ“Š <strong>3x memory savings</strong> using QLoRA (4-bit quantization)</p>
<h2 id="wb-experiment-tracking" class="relative group">W&amp;B Experiment Tracking <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#wb-experiment-tracking" aria-label="Anchor">#</a></span></h2><p>All experiments logged to Weights &amp; Biases:</p>
<ul>
<li>Loss curves and learning rate schedules</li>
<li>Validation metrics every 500 steps</li>
<li>Hyperparameter sweeps (rank, alpha, dropout)</li>
<li>GPU memory usage and throughput</li>
</ul>
<p>ðŸ”— <strong><a href="https://wandb.ai/0xmuler/llm-finetuning" target="_blank" rel="noreferrer">View W&amp;B Dashboard â†’</a></strong></p>
<h2 id="deployment" class="relative group">Deployment <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#deployment" aria-label="Anchor">#</a></span></h2><h3 id="hugging-face-hub" class="relative group">Hugging Face Hub <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#hugging-face-hub" aria-label="Anchor">#</a></span></h3><p>Fine-tuned models and adapters published:</p>
<p>ðŸ¤— <strong><a href="https://huggingface.co/0xmuler/mistral-medical-lora" target="_blank" rel="noreferrer">0xmuler/mistral-medical-lora â†’</a></strong><br>
ðŸ¤— <strong><a href="https://huggingface.co/0xmuler/llama3-legal-qlora" target="_blank" rel="noreferrer">0xmuler/llama3-legal-qlora â†’</a></strong></p>
<h3 id="inference-api" class="relative group">Inference API <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#inference-api" aria-label="Anchor">#</a></span></h3><p>FastAPI endpoint with Docker:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>docker run -p 8000:8000 0xmuler/llm-inference-api
</span></span><span style="display:flex;"><span>curl -X POST http://localhost:8000/generate <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span>  -H <span style="color:#f1fa8c">&#34;Content-Type: application/json&#34;</span> <span style="color:#f1fa8c">\
</span></span></span><span style="display:flex;"><span>  -d <span style="color:#f1fa8c">&#39;{&#34;prompt&#34;: &#34;Analyze this contract...&#34;}&#39;</span>
</span></span></code></pre></div><h2 id="github-repository" class="relative group">GitHub Repository <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#github-repository" aria-label="Anchor">#</a></span></h2><p>ðŸ“‚ <strong><a href="https://github.com/0xmuler/llm-finetuning" target="_blank" rel="noreferrer">View Source Code â†’</a></strong></p>
<p>Includes:</p>
<ul>
<li>Training scripts with configurable hyperparameters</li>
<li>Data preprocessing pipelines</li>
<li>Evaluation scripts (perplexity, domain metrics)</li>
<li>Inference server with FastAPI</li>
<li>Comprehensive README and setup guide</li>
</ul>
<h2 id="challenges--solutions" class="relative group">Challenges &amp; Solutions <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#challenges--solutions" aria-label="Anchor">#</a></span></h2><table>
  <thead>
      <tr>
          <th>Challenge</th>
          <th>Solution</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>GPU memory limits</strong></td>
          <td>QLoRA 4-bit quantization</td>
      </tr>
      <tr>
          <td><strong>Overfitting on small datasets</strong></td>
          <td>Aggressive dropout, early stopping</td>
      </tr>
      <tr>
          <td><strong>Slow training</strong></td>
          <td>Gradient checkpointing, mixed precision</td>
      </tr>
      <tr>
          <td><strong>Catastrophic forgetting</strong></td>
          <td>Regularization, held-out general benchmark</td>
      </tr>
  </tbody>
</table>
<h2 id="key-learnings" class="relative group">Key Learnings <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#key-learnings" aria-label="Anchor">#</a></span></h2><ol>
<li><strong>LoRA rank tuning matters</strong> â€” r=16 optimal for most tasks, r=64 for complex domains</li>
<li><strong>Data quality &gt; quantity</strong> â€” 5k high-quality examples beat 50k noisy ones</li>
<li><strong>Evaluation is hard</strong> â€” Domain-specific metrics more valuable than perplexity</li>
<li><strong>Deployment is 50% of the work</strong> â€” Quantization, batching, caching for production</li>
</ol>
<h2 id="next-steps" class="relative group">Next Steps <span class="absolute top-0 w-6 transition-opacity opacity-0 -start-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style="text-decoration-line: none !important;" href="#next-steps" aria-label="Anchor">#</a></span></h2><ul>
<li><strong>Multi-task LoRA</strong> â€” Train adapters for multiple domains simultaneously</li>
<li><strong>Instruction tuning</strong> â€” Fine-tune on instruction-following datasets</li>
<li><strong>RLHF</strong> â€” Reinforcement learning from human feedback</li>
<li><strong>Distillation</strong> â€” Compress fine-tuned model for edge deployment</li>
</ul>

      </div>
    </section>
    <footer class="max-w-prose pt-8 print:hidden">
      
  <div class="flex">
    
    
    
      
      
    
    <div class="place-self-center">
      
        <div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">
          Author
        </div>
        <div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">
          0xsersif
        </div>
      
      
        <div class="text-sm text-neutral-700 dark:text-neutral-400">Full-Stack Developer &amp; ML Engineer. Building web applications, websites, and production AI systems. 5+ years self-learning. HTML &amp; CSS certified.</div>
      
      <div class="text-2xl sm:text-lg">
</div>
    </div>
  </div>


      

      
  
    
    
    
    <div class="pt-8">
      <hr class="border-dotted border-neutral-300 dark:border-neutral-600" />
      <div class="flex justify-between pt-3">
        <span>
          
            <a class="group flex" href="http://localhost:1313/projects/rag-system/">
              <span
                class="me-2 text-neutral-700 transition-transform group-hover:-translate-x-[2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&larr;</span
                ><span class="ltr:hidden rtl:inline">&rarr;</span></span
              >
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >RAG System with LangChain & Vector Databases</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2026-01-15 00:00:00 &#43;0000 UTC">15 January 2026</time>
                  
                </span>
              </span>
            </a>
          
        </span>
        <span>
          
            <a class="group flex text-right" href="http://localhost:1313/projects/multi-agent-workflow/">
              <span class="flex flex-col">
                <span
                  class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500"
                  >Multi-Agent AI Workflow System</span
                >
                <span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400">
                  
                    <time datetime="2026-01-25 00:00:00 &#43;0000 UTC">25 January 2026</time>
                  
                </span>
              </span>
              <span
                class="ms-2 text-neutral-700 transition-transform group-hover:-translate-x-[-2px] group-hover:text-primary-600 dark:text-neutral dark:group-hover:text-primary-400"
                ><span class="ltr:inline rtl:hidden">&rarr;</span
                ><span class="ltr:hidden rtl:inline">&larr;</span></span
              >
            </a>
          
        </span>
      </div>
    </div>
  


      
    </footer>
  </article>

      </main>
      
        <div
          class="pointer-events-none absolute bottom-0 end-0 top-[100vh] w-12"
          id="to-top"
          hidden="true"
        >
          <a
            href="#the-top"
            class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400"
            aria-label="Scroll to top"
            title="Scroll to top"
          >
            &uarr;
          </a>
        </div>
      <footer class="py-10 print:hidden">
  
  
  <div class="flex items-center justify-between">
    <div>
      
      
        <p class="text-sm text-neutral-500 dark:text-neutral-400">
            &copy;
            2026
            0xsersif
        </p>
      
      
      
        <p class="text-xs text-neutral-500 dark:text-neutral-400">
          
          
          Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500"
            href="https://gohugo.io/" target="_blank" rel="noopener noreferrer">Hugo</a> &amp; <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href="https://github.com/jpanther/congo" target="_blank" rel="noopener noreferrer">Congo</a>
        </p>
      
    </div>
    <div class="flex flex-row items-center">
      
      
      
      
    </div>
  </div>
  
  
</footer>

    </div>
  </body>
</html>
