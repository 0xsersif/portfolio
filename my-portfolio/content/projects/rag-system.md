---
title: "RAG System with LangChain & Vector Databases"
date: 2026-01-15
description: "Production RAG system answering questions from private document sets using LangChain, ChromaDB, and FastAPI"
tags: ["MLOps", "LLM", "RAG", "LangChain", "Vector DB", "FastAPI"]
categories: ["MLOps", "LLM Engineering"]
---

# RAG System with LangChain & LlamaIndex

## Problem Statement

Build a **Retrieval-Augmented Generation (RAG)** chatbot that accurately answers questions based on a private, custom document set (PDFs, Notion exports, Slack conversations) while minimizing hallucinations.

## Architecture

```
Documents â†’ Chunking â†’ Embeddings â†’ Vector DB (ChromaDB/Pinecone)
                                            â†“
User Query â†’ Embedding â†’ Similarity Search â†’ Context Retrieval
                                            â†“
                              LLM (GPT-4/Claude) + Context â†’ Answer
```

### Key Components

1. **Document Processing**: Parse PDFs, extract text, chunk with semantic overlap
2. **Vector Database**: ChromaDB for local development, Pinecone for production scale
3. **Orchestration**: LangChain for prompt engineering and chain management
4. **API Layer**: FastAPI for RESTful interface
5. **Containerization**: Docker for reproducible deployments

## Tech Stack

- **Python** â€” Core language
- **LangChain** â€” LLM orchestration and prompt management
- **LlamaIndex** â€” Document indexing and retrieval
- **ChromaDB / Pinecone** â€” Vector storage
- **FAISS** â€” Efficient similarity search
- **FastAPI** â€” API endpoint
- **Docker** â€” Containerization
- **OpenAI API** â€” GPT-4 for generation

## Key Features

âœ… **Chunking Strategies** â€” Tested semantic, fixed-size, and recursive chunking  
âœ… **Hybrid Search** â€” Combined dense (embeddings) + sparse (BM25) retrieval  
âœ… **Source Attribution** â€” Answers cite source documents with page numbers  
âœ… **Hallucination Detection** â€” Confidence scoring and answer validation  
âœ… **Fast Retrieval** â€” Optimized indexing for <500ms query latency

## Impact & Metrics

ðŸ“Š **Reduced retrieval latency by 40%** through FAISS optimization  
ðŸ“Š **85% answer accuracy** validated against golden Q&A dataset  
ðŸ“Š **Zero hallucinations** on out-of-scope questions (returns "I don't know")

## Live Demo

ðŸš€ **[Try it on Hugging Face Spaces â†’](https://huggingface.co/spaces/0xmuler/rag-demo)**

## GitHub Repository

ðŸ“‚ **[View Source Code â†’](https://github.com/0xmuler/rag-system)**

Features:
- Professional README with setup instructions
- Architecture diagram
- Docker Compose for one-command deployment
- Evaluation scripts comparing chunking strategies
- CI/CD pipeline with GitHub Actions

## Lessons Learned

1. **Chunking matters** â€” Semantic chunking improved accuracy by 12% vs fixed-size
2. **Hybrid search wins** â€” Dense + sparse retrieval outperformed pure embedding search
3. **Prompt engineering is critical** â€” Iterative prompt refinement reduced hallucinations by 30%
4. **Real-world data is messy** â€” Spent 40% of time on PDF parsing edge cases

## Future Enhancements

- Multi-modal RAG (images, tables, charts)
- Fine-tune embeddings on domain-specific corpus
- Add streaming responses for better UX
- Implement query rewriting for complex questions
