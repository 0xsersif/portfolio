---
title: "LLM Evaluation Dashboard"
date: 2026-01-28
description: "Dashboard for evaluating RAG systems, comparing chunking strategies, and minimizing hallucinations"
tags: ["MLOps", "LLM", "Evaluation", "Streamlit", "MLflow"]
categories: ["MLOps", "LLM Engineering"]
---

# LLM Evaluation Dashboard

## Problem Statement

RAG systems are hard to evaluate. How do you:
- Compare different chunking strategies (semantic vs fixed-size)?
- Measure hallucination rates across prompt variations?
- A/B test retrieval methods (dense vs hybrid)?
- Track performance over time?

This project builds a **comprehensive evaluation framework** with visual dashboards to systematically improve RAG system quality.

## Architecture

```
Evaluation Dataset (Golden Q&A Pairs)
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Test Configurations â”‚
â”‚ - Chunking: 3 types   â”‚
â”‚ - Prompts: 5 variants â”‚
â”‚ - Retrieval: 2 methodsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
    Run Experiments (MLflow)
            â†“
    Compute Metrics
    - Answer accuracy
    - Hallucination rate  
    - Retrieval precision
    - Latency
            â†“
    Streamlit Dashboard
    (Interactive Comparison)
```

## Tech Stack

- **Streamlit** â€” Interactive dashboard UI
- **MLflow** â€” Experiment tracking and runs comparison
- **LangChain** â€” RAG system orchestration
- **pytest** â€” Automated evaluation tests
- **pandas** â€” Data manipulation and analysis
- **plotly** â€” Interactive visualizations
- **Docker** â€” Reproducible environment

## Evaluation Metrics

### 1. Answer Accuracy
Compare generated answer to golden answer:
- **Exact match** â€” Binary correctness
- **BLEU score** â€” N-gram overlap
- **ROUGE score** â€” Recall-oriented similarity
- **Semantic similarity** â€” Embedding cosine distance

### 2. Hallucination Detection
Check if answer contains hallucinated information:
- **Source attribution** â€” All facts cite retrieved documents
- **Out-of-scope detection** â€” Correctly says "I don't know"
- **Factual consistency** â€” NLI model checks answer vs sources

### 3. Retrieval Quality
Evaluate document retrieval:
- **Precision@K** â€” Relevant docs in top K
- **Recall@K** â€” Coverage of all relevant docs
- **MRR (Mean Reciprocal Rank)** â€” Position of first relevant doc
- **Latency** â€” Time to retrieve + rank documents

## Tested Configurations

### Chunking Strategies
1. **Fixed-size** â€” 512 tokens, 50 token overlap
2. **Semantic** â€” Split by paragraph/section headers
3. **Recursive** â€” Hierarchical chunking with parent-child

### Prompt Variations
1. **Zero-shot** â€” Direct question answering
2. **Few-shot** â€” 3 examples in prompt
3. **Chain-of-thought** â€” "Think step-by-step..."
4. **Constrained** â€” "Answer using only provided context"
5. **Structured** â€” JSON output format

### Retrieval Methods
1. **Dense** â€” Pure embedding similarity (FAISS)
2. **Hybrid** â€” Dense + BM25 sparse retrieval

## Dashboard Features

### Experiment Comparison View
- Side-by-side metrics for all configurations
- Interactive filters (chunking, prompt, retrieval)
- Sort by any metric (accuracy, latency, etc.)

### Hallucination Analysis
- Per-question hallucination breakdown
- Examples of hallucinated answers
- Confidence score distribution

### Latency Profiling
- P50, P95, P99 latency percentiles
- Breakdown: retrieval vs generation time
- Identify slow queries

### Error Analysis
- Failed questions grouped by topic
- Common failure patterns
- Suggested improvements

## Results & Insights

ğŸ“Š **Semantic chunking improved accuracy by 12%** vs fixed-size  
ğŸ“Š **Hybrid retrieval reduced hallucinations by 35%**  
ğŸ“Š **Constrained prompts cut hallucination rate from 18% â†’ 6%**  
ğŸ“Š **FAISS optimization reduced latency from 800ms â†’ 400ms**

### Winning Configuration

| Component | Best Approach |
|-----------|---------------|
| **Chunking** | Semantic (paragraph boundaries) |
| **Prompt** | Constrained + few-shot examples |
| **Retrieval** | Hybrid (dense + BM25) |
| **Top-K** | 5 documents |
| **Re-ranking** | Cross-encoder final pass |

**Final Metrics**:  
âœ… 85% answer accuracy  
âœ… 6% hallucination rate  
âœ… 420ms average latency

## Live Demo

ğŸš€ **[Try the Dashboard on Streamlit Cloud â†’](https://0xmuler-llm-eval.streamlit.app)**

Upload your own evaluation dataset and run experiments.

## GitHub Repository

ğŸ“‚ **[View Source Code â†’](https://github.com/0xmuler/llm-evaluation-dashboard)**

Includes:
- Full evaluation framework
- Golden Q&A dataset (100+ examples)
- MLflow experiment tracking setup
- Streamlit dashboard code
- Automated pytest suite
- Docker Compose for one-command deployment

## Example Evaluation Run

```python
from llm_eval import RAGEvaluator

evaluator = RAGEvaluator(
    dataset="golden_qa.json",
    chunking="semantic",
    prompt_template="constrained",
    retrieval="hybrid"
)

results = evaluator.run()
# Logs to MLflow automatically

print(results.summary())
# Accuracy: 0.85, Hallucination: 0.06, Latency: 420ms
```

## Automated Testing

CI/CD pipeline (GitHub Actions) runs on every PR:

```yaml
- Run evaluation on 20-question smoke test
- Fail if accuracy < 75%
- Fail if hallucination > 10%
- Fail if P95 latency > 1000ms
```

## Key Learnings

1. **Golden datasets are essential** â€” Can't improve what you don't measure
2. **Metrics must match use case** â€” BLEU â‰  semantic correctness
3. **Ablation studies reveal insights** â€” Change one variable at a time
4. **Latency-quality tradeoffs** â€” Faster retrieval may hurt accuracy
5. **Visualizations drive decisions** â€” Dashboard convinced stakeholders

## Challenges & Solutions

| Challenge | Solution |
|-----------|----------|
| **Creating golden dataset** | Hired domain experts for annotation |
| **Expensive LLM calls** | Cache LLM responses, use cheaper models for eval |
| **Evaluation is subjective** | Multiple annotators, inter-rater agreement |
| **Complex UI** | Streamlit with clear filters and tooltips |

## Future Enhancements

- **Multi-modal evaluation** â€” Test with images, tables, charts
- **Adversarial testing** â€” Inject misleading docs, test robustness
- **Online evaluation** â€” Monitor production queries automatically
- **Cost tracking** â€” Compare configurations by $/1000 queries
- **Custom metrics** â€” Domain-specific evaluation criteria
